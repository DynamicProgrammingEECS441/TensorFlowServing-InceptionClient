// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow/core/protobuf/config.proto

// This CPP symbol can be defined to use imports that match up to the framework
// imports needed when using CocoaPods.
#if !defined(GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS)
 #define GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS 0
#endif

#if GPB_USE_PROTOBUF_FRAMEWORK_IMPORTS
 #import <Protobuf/GPBProtocolBuffers.h>
#else
 #import "GPBProtocolBuffers.h"
#endif

#if GOOGLE_PROTOBUF_OBJC_VERSION < 30002
#error This file was generated by a newer version of protoc which is incompatible with your Protocol Buffer library sources.
#endif
#if 30002 < GOOGLE_PROTOBUF_OBJC_MIN_SUPPORTED_VERSION
#error This file was generated by an older version of protoc which is incompatible with your Protocol Buffer library sources.
#endif

// @@protoc_insertion_point(imports)

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdeprecated-declarations"

CF_EXTERN_C_BEGIN

@class ClusterDef;
@class CostGraphDef;
@class DebugOptions;
@class GPUOptions;
@class GraphDef;
@class GraphOptions;
@class OptimizerOptions;
@class RPCOptions;
@class RewriterConfig;
@class StepStats;
@class ThreadPoolOptionProto;

NS_ASSUME_NONNULL_BEGIN

#pragma mark - Enum OptimizerOptions_Level

/** Optimization level */
typedef GPB_ENUM(OptimizerOptions_Level) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  OptimizerOptions_Level_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /**
   * L1 is the default level.
   * Optimization performed at L1 :
   * 1. Common subexpression elimination
   * 2. Constant folding
   **/
  OptimizerOptions_Level_L1 = 0,

  /** No optimizations */
  OptimizerOptions_Level_L0 = -1,
};

GPBEnumDescriptor *OptimizerOptions_Level_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL OptimizerOptions_Level_IsValidValue(int32_t value);

#pragma mark - Enum OptimizerOptions_GlobalJitLevel

/** Control the use of the compiler/jit.  Experimental. */
typedef GPB_ENUM(OptimizerOptions_GlobalJitLevel) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  OptimizerOptions_GlobalJitLevel_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  /** Default setting ("off" now, but later expected to be "on") */
  OptimizerOptions_GlobalJitLevel_Default = 0,
  OptimizerOptions_GlobalJitLevel_Off = -1,

  /**
   * The following settings turn on compilation, with higher values being
   * more aggressive.  Higher values may reduce opportunities for parallelism
   * and may use more memory.  (At present, there is no distinction, but this
   * is expected to change.)
   **/
  OptimizerOptions_GlobalJitLevel_On1 = 1,
  OptimizerOptions_GlobalJitLevel_On2 = 2,
};

GPBEnumDescriptor *OptimizerOptions_GlobalJitLevel_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL OptimizerOptions_GlobalJitLevel_IsValidValue(int32_t value);

#pragma mark - Enum RunOptions_TraceLevel

/**
 * TODO(pbar) Turn this into a TraceOptions proto which allows
 * tracing to be controlled in a more orthogonal manner?
 **/
typedef GPB_ENUM(RunOptions_TraceLevel) {
  /**
   * Value used if any message's field encounters a value that is not defined
   * by this enum. The message will also have C functions to get/set the rawValue
   * of the field.
   **/
  RunOptions_TraceLevel_GPBUnrecognizedEnumeratorValue = kGPBUnrecognizedEnumeratorValue,
  RunOptions_TraceLevel_NoTrace = 0,
  RunOptions_TraceLevel_SoftwareTrace = 1,
  RunOptions_TraceLevel_HardwareTrace = 2,
  RunOptions_TraceLevel_FullTrace = 3,
};

GPBEnumDescriptor *RunOptions_TraceLevel_EnumDescriptor(void);

/**
 * Checks to see if the given value is defined by the enum or was not known at
 * the time this source was generated.
 **/
BOOL RunOptions_TraceLevel_IsValidValue(int32_t value);

#pragma mark - ConfigRoot

/**
 * Exposes the extension registry for this file.
 *
 * The base class provides:
 * @code
 *   + (GPBExtensionRegistry *)extensionRegistry;
 * @endcode
 * which is a @c GPBExtensionRegistry that includes all the extensions defined by
 * this file and all files that it depends on.
 **/
@interface ConfigRoot : GPBRootObject
@end

#pragma mark - GPUOptions

typedef GPB_ENUM(GPUOptions_FieldNumber) {
  GPUOptions_FieldNumber_PerProcessGpuMemoryFraction = 1,
  GPUOptions_FieldNumber_AllocatorType = 2,
  GPUOptions_FieldNumber_DeferredDeletionBytes = 3,
  GPUOptions_FieldNumber_AllowGrowth = 4,
  GPUOptions_FieldNumber_VisibleDeviceList = 5,
  GPUOptions_FieldNumber_PollingActiveDelayUsecs = 6,
  GPUOptions_FieldNumber_PollingInactiveDelayMsecs = 7,
  GPUOptions_FieldNumber_ForceGpuCompatible = 8,
};

@interface GPUOptions : GPBMessage

/**
 * A value between 0 and 1 that indicates what fraction of the
 * available GPU memory to pre-allocate for each process.  1 means
 * to pre-allocate all of the GPU memory, 0.5 means the process
 * allocates ~50% of the available GPU memory.
 **/
@property(nonatomic, readwrite) double perProcessGpuMemoryFraction;

/**
 * The type of GPU allocation strategy to use.
 *
 * Allowed values:
 * "": The empty string (default) uses a system-chosen default
 *     which may change over time.
 *
 * "BFC": A "Best-fit with coalescing" algorithm, simplified from a
 *        version of dlmalloc.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *allocatorType;

/**
 * Delay deletion of up to this many bytes to reduce the number of
 * interactions with gpu driver code.  If 0, the system chooses
 * a reasonable default (several MBs).
 **/
@property(nonatomic, readwrite) int64_t deferredDeletionBytes;

/**
 * If true, the allocator does not pre-allocate the entire specified
 * GPU memory region, instead starting small and growing as needed.
 **/
@property(nonatomic, readwrite) BOOL allowGrowth;

/**
 * A comma-separated list of GPU ids that determines the 'visible'
 * to 'virtual' mapping of GPU devices.  For example, if TensorFlow
 * can see 8 GPU devices in the process, and one wanted to map
 * visible GPU devices 5 and 3 as "/gpu:0", and "/gpu:1", then one
 * would specify this field as "5,3".  This field is similar in
 * spirit to the CUDA_VISIBLE_DEVICES environment variable, except
 * it applies to the visible GPU devices in the process.
 *
 * NOTE: The GPU driver provides the process with the visible GPUs
 * in an order which is not guaranteed to have any correlation to
 * the *physical* GPU id in the machine.  This field is used for
 * remapping "visible" to "virtual", which means this operates only
 * after the process starts.  Users are required to use vendor
 * specific mechanisms (e.g., CUDA_VISIBLE_DEVICES) to control the
 * physical to visible device mapping prior to invoking TensorFlow.
 **/
@property(nonatomic, readwrite, copy, null_resettable) NSString *visibleDeviceList;

/**
 * In the event polling loop sleep this many microseconds between
 * PollEvents calls, when the queue is not empty.  If value is not
 * set or set to 0, gets set to a non-zero default.
 **/
@property(nonatomic, readwrite) int32_t pollingActiveDelayUsecs;

/**
 * In the event polling loop sleep this many millisconds between
 * PollEvents calls, when the queue is empty.  If value is not
 * set or set to 0, gets set to a non-zero default.
 **/
@property(nonatomic, readwrite) int32_t pollingInactiveDelayMsecs;

/**
 * Force all tensors to be gpu_compatible. On a GPU-enabled TensorFlow,
 * enabling this option forces all CPU tensors to be allocated with Cuda
 * pinned memory. Normally, TensorFlow will infer which tensors should be
 * allocated as the pinned memory. But in case where the inference is
 * incomplete, this option can significantly speed up the cross-device memory
 * copy performance as long as it fits the memory.
 * Note that this option is not something that should be
 * enabled by default for unknown or very large models, since all Cuda pinned
 * memory is unpageable, having too much pinned memory might negatively impact
 * the overall host system performance.
 **/
@property(nonatomic, readwrite) BOOL forceGpuCompatible;

@end

#pragma mark - OptimizerOptions

typedef GPB_ENUM(OptimizerOptions_FieldNumber) {
  OptimizerOptions_FieldNumber_DoCommonSubexpressionElimination = 1,
  OptimizerOptions_FieldNumber_DoConstantFolding = 2,
  OptimizerOptions_FieldNumber_OptLevel = 3,
  OptimizerOptions_FieldNumber_DoFunctionInlining = 4,
  OptimizerOptions_FieldNumber_GlobalJitLevel = 5,
};

/**
 * Options passed to the graph optimizer
 **/
@interface OptimizerOptions : GPBMessage

/** If true, optimize the graph using common subexpression elimination. */
@property(nonatomic, readwrite) BOOL doCommonSubexpressionElimination;

/** If true, perform constant folding optimization on the graph. */
@property(nonatomic, readwrite) BOOL doConstantFolding;

/** If true, perform function inlining on the graph. */
@property(nonatomic, readwrite) BOOL doFunctionInlining;

@property(nonatomic, readwrite) OptimizerOptions_Level optLevel;

@property(nonatomic, readwrite) OptimizerOptions_GlobalJitLevel globalJitLevel;

@end

/**
 * Fetches the raw value of a @c OptimizerOptions's @c optLevel property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t OptimizerOptions_OptLevel_RawValue(OptimizerOptions *message);
/**
 * Sets the raw value of an @c OptimizerOptions's @c optLevel property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetOptimizerOptions_OptLevel_RawValue(OptimizerOptions *message, int32_t value);

/**
 * Fetches the raw value of a @c OptimizerOptions's @c globalJitLevel property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t OptimizerOptions_GlobalJitLevel_RawValue(OptimizerOptions *message);
/**
 * Sets the raw value of an @c OptimizerOptions's @c globalJitLevel property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetOptimizerOptions_GlobalJitLevel_RawValue(OptimizerOptions *message, int32_t value);

#pragma mark - GraphOptions

typedef GPB_ENUM(GraphOptions_FieldNumber) {
  GraphOptions_FieldNumber_EnableRecvScheduling = 2,
  GraphOptions_FieldNumber_OptimizerOptions = 3,
  GraphOptions_FieldNumber_BuildCostModel = 4,
  GraphOptions_FieldNumber_InferShapes = 5,
  GraphOptions_FieldNumber_PlacePrunedGraph = 6,
  GraphOptions_FieldNumber_EnableBfloat16Sendrecv = 7,
  GraphOptions_FieldNumber_TimelineStep = 8,
  GraphOptions_FieldNumber_BuildCostModelAfter = 9,
  GraphOptions_FieldNumber_RewriteOptions = 10,
};

@interface GraphOptions : GPBMessage

/**
 * If true, use control flow to schedule the activation of Recv nodes.
 * (Currently ignored.)
 **/
@property(nonatomic, readwrite) BOOL enableRecvScheduling;

/** Options controlling how graph is optimized. */
@property(nonatomic, readwrite, strong, null_resettable) OptimizerOptions *optimizerOptions;
/** Test to see if @c optimizerOptions has been set. */
@property(nonatomic, readwrite) BOOL hasOptimizerOptions;

/**
 * The number of steps to run before returning a cost model detailing
 * the memory usage and performance of each node of the graph. 0 means
 * no cost model.
 **/
@property(nonatomic, readwrite) int64_t buildCostModel;

/**
 * The number of steps to skip before collecting statistics for the
 * cost model.
 **/
@property(nonatomic, readwrite) int64_t buildCostModelAfter;

/**
 * Annotate each Node with Op output shape data, to the extent it can
 * be statically inferred.
 **/
@property(nonatomic, readwrite) BOOL inferShapes;

/**
 * Only place the subgraphs that are run, rather than the entire graph.
 *
 * This is useful for interactive graph building, where one might
 * produce graphs that cannot be placed during the debugging
 * process.  In particular, it allows the client to continue work in
 * a session after adding a node to a graph whose placement
 * constraints are unsatisfiable.
 **/
@property(nonatomic, readwrite) BOOL placePrunedGraph;

/** If true, transfer float values between processes as bfloat16. */
@property(nonatomic, readwrite) BOOL enableBfloat16Sendrecv;

/**
 * If > 0, record a timeline every this many steps.
 * EXPERIMENTAL: This currently has no effect in MasterSession.
 **/
@property(nonatomic, readwrite) int32_t timelineStep;

/** Options that control the type and amount of graph rewriting. */
@property(nonatomic, readwrite, strong, null_resettable) RewriterConfig *rewriteOptions;
/** Test to see if @c rewriteOptions has been set. */
@property(nonatomic, readwrite) BOOL hasRewriteOptions;

@end

#pragma mark - ThreadPoolOptionProto

typedef GPB_ENUM(ThreadPoolOptionProto_FieldNumber) {
  ThreadPoolOptionProto_FieldNumber_NumThreads = 1,
};

@interface ThreadPoolOptionProto : GPBMessage

/**
 * The number of threads in the pool.
 *
 * 0 means the system picks a value based on where this option proto is used
 * (see the declaration of the specific field for more info).
 **/
@property(nonatomic, readwrite) int32_t numThreads;

@end

#pragma mark - RPCOptions

typedef GPB_ENUM(RPCOptions_FieldNumber) {
  RPCOptions_FieldNumber_UseRpcForInprocessMaster = 1,
};

@interface RPCOptions : GPBMessage

/**
 * If true, always use RPC to contact the session target.
 *
 * If false (the default option), TensorFlow may use an optimized
 * transport for client-master communication that avoids the RPC
 * stack. This option is primarily for used testing the RPC stack.
 **/
@property(nonatomic, readwrite) BOOL useRpcForInprocessMaster;

@end

#pragma mark - ConfigProto

typedef GPB_ENUM(ConfigProto_FieldNumber) {
  ConfigProto_FieldNumber_DeviceCount = 1,
  ConfigProto_FieldNumber_IntraOpParallelismThreads = 2,
  ConfigProto_FieldNumber_PlacementPeriod = 3,
  ConfigProto_FieldNumber_DeviceFiltersArray = 4,
  ConfigProto_FieldNumber_InterOpParallelismThreads = 5,
  ConfigProto_FieldNumber_GpuOptions = 6,
  ConfigProto_FieldNumber_AllowSoftPlacement = 7,
  ConfigProto_FieldNumber_LogDevicePlacement = 8,
  ConfigProto_FieldNumber_UsePerSessionThreads = 9,
  ConfigProto_FieldNumber_GraphOptions = 10,
  ConfigProto_FieldNumber_OperationTimeoutInMs = 11,
  ConfigProto_FieldNumber_SessionInterOpThreadPoolArray = 12,
  ConfigProto_FieldNumber_RpcOptions = 13,
  ConfigProto_FieldNumber_ClusterDef = 14,
};

/**
 * Session configuration parameters.
 * The system picks appropriate values for fields that are not set.
 **/
@interface ConfigProto : GPBMessage

/**
 * Map from device type name (e.g., "CPU" or "GPU" ) to maximum
 * number of devices of that type to use.  If a particular device
 * type is not found in the map, the system picks an appropriate
 * number.
 **/
@property(nonatomic, readwrite, strong, null_resettable) GPBStringInt32Dictionary *deviceCount;
/** The number of items in @c deviceCount without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger deviceCount_Count;

/**
 * The execution of an individual op (for some op types) can be
 * parallelized on a pool of intra_op_parallelism_threads.
 * 0 means the system picks an appropriate number.
 **/
@property(nonatomic, readwrite) int32_t intraOpParallelismThreads;

/**
 * Nodes that perform blocking operations are enqueued on a pool of
 * inter_op_parallelism_threads available in each process.
 *
 * 0 means the system picks an appropriate number.
 *
 * Note that the first Session created in the process sets the
 * number of threads for all future sessions unless use_per_session_threads is
 * true or session_inter_op_thread_pool is configured.
 **/
@property(nonatomic, readwrite) int32_t interOpParallelismThreads;

/**
 * If true, use a new set of threads for this session rather than the global
 * pool of threads. Only supported by direct sessions.
 *
 * If false, use the global threads created by the first session, or the
 * per-session thread pools configured by session_inter_op_thread_pool.
 *
 * This option is deprecated. The same effect can be achieved by setting
 * session_inter_op_thread_pool to have one element, whose num_threads equals
 * inter_op_parallelism_threads.
 **/
@property(nonatomic, readwrite) BOOL usePerSessionThreads;

/**
 * This option is experimental - it may be replaced with a different mechanism
 * in the future. The intended use is for when some session invocations need
 * to run in a background pool limited to a small number of threads.
 *
 * Configures session thread pools. If this is configured, then RunOptions for
 * a Run call can select the thread pool to use.
 *
 * If a pool's num_threads is 0, then inter_op_parallelism_threads is used.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<ThreadPoolOptionProto*> *sessionInterOpThreadPoolArray;
/** The number of items in @c sessionInterOpThreadPoolArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger sessionInterOpThreadPoolArray_Count;

/**
 * Assignment of Nodes to Devices is recomputed every placement_period
 * steps until the system warms up (at which point the recomputation
 * typically slows down automatically).
 **/
@property(nonatomic, readwrite) int32_t placementPeriod;

/**
 * When any filters are present sessions will ignore all devices which do not
 * match the filters. Each filter can be partially specified, e.g. "/job:ps"
 * "/job:worker/replica:3", etc.
 **/
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<NSString*> *deviceFiltersArray;
/** The number of items in @c deviceFiltersArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger deviceFiltersArray_Count;

/** Options that apply to all GPUs. */
@property(nonatomic, readwrite, strong, null_resettable) GPUOptions *gpuOptions;
/** Test to see if @c gpuOptions has been set. */
@property(nonatomic, readwrite) BOOL hasGpuOptions;

/**
 * Whether soft placement is allowed. If allow_soft_placement is true,
 * an op will be placed on CPU if
 *   1. there's no GPU implementation for the OP
 * or
 *   2. no GPU devices are known or registered
 * or
 *   3. need to co-locate with reftype input(s) which are from CPU.
 **/
@property(nonatomic, readwrite) BOOL allowSoftPlacement;

/** Whether device placements should be logged. */
@property(nonatomic, readwrite) BOOL logDevicePlacement;

/** Options that apply to all graphs. */
@property(nonatomic, readwrite, strong, null_resettable) GraphOptions *graphOptions;
/** Test to see if @c graphOptions has been set. */
@property(nonatomic, readwrite) BOOL hasGraphOptions;

/**
 * Global timeout for all blocking operations in this session.  If non-zero,
 * and not overridden on a per-operation basis, this value will be used as the
 * deadline for all blocking operations.
 **/
@property(nonatomic, readwrite) int64_t operationTimeoutInMs;

/** Options that apply when this session uses the distributed runtime. */
@property(nonatomic, readwrite, strong, null_resettable) RPCOptions *rpcOptions;
/** Test to see if @c rpcOptions has been set. */
@property(nonatomic, readwrite) BOOL hasRpcOptions;

/** Optional list of all workers to use in this session. */
@property(nonatomic, readwrite, strong, null_resettable) ClusterDef *clusterDef;
/** Test to see if @c clusterDef has been set. */
@property(nonatomic, readwrite) BOOL hasClusterDef;

@end

#pragma mark - RunOptions

typedef GPB_ENUM(RunOptions_FieldNumber) {
  RunOptions_FieldNumber_TraceLevel = 1,
  RunOptions_FieldNumber_TimeoutInMs = 2,
  RunOptions_FieldNumber_InterOpThreadPool = 3,
  RunOptions_FieldNumber_OutputPartitionGraphs = 5,
  RunOptions_FieldNumber_DebugOptions = 6,
};

/**
 * Options for a single Run() call.
 **/
@interface RunOptions : GPBMessage

@property(nonatomic, readwrite) RunOptions_TraceLevel traceLevel;

/** Time to wait for operation to complete in milliseconds. */
@property(nonatomic, readwrite) int64_t timeoutInMs;

/** The thread pool to use, if session_inter_op_thread_pool is configured. */
@property(nonatomic, readwrite) int32_t interOpThreadPool;

/**
 * Whether the partition graph(s) executed by the executor(s) should be
 * outputted via RunMetadata.
 **/
@property(nonatomic, readwrite) BOOL outputPartitionGraphs;

/** EXPERIMENTAL.  Options used to initialize DebuggerState, if enabled. */
@property(nonatomic, readwrite, strong, null_resettable) DebugOptions *debugOptions;
/** Test to see if @c debugOptions has been set. */
@property(nonatomic, readwrite) BOOL hasDebugOptions;

@end

/**
 * Fetches the raw value of a @c RunOptions's @c traceLevel property, even
 * if the value was not defined by the enum at the time the code was generated.
 **/
int32_t RunOptions_TraceLevel_RawValue(RunOptions *message);
/**
 * Sets the raw value of an @c RunOptions's @c traceLevel property, allowing
 * it to be set to a value that was not defined by the enum at the time the code
 * was generated.
 **/
void SetRunOptions_TraceLevel_RawValue(RunOptions *message, int32_t value);

#pragma mark - RunMetadata

typedef GPB_ENUM(RunMetadata_FieldNumber) {
  RunMetadata_FieldNumber_StepStats = 1,
  RunMetadata_FieldNumber_CostGraph = 2,
  RunMetadata_FieldNumber_PartitionGraphsArray = 3,
};

/**
 * Metadata output (i.e., non-Tensor) for a single Run() call.
 **/
@interface RunMetadata : GPBMessage

/**
 * Statistics traced for this step. Populated if tracing is turned on via the
 * "RunOptions" proto.
 * EXPERIMENTAL: The format and set of events may change in future versions.
 **/
@property(nonatomic, readwrite, strong, null_resettable) StepStats *stepStats;
/** Test to see if @c stepStats has been set. */
@property(nonatomic, readwrite) BOOL hasStepStats;

/** The cost graph for the computation defined by the run call. */
@property(nonatomic, readwrite, strong, null_resettable) CostGraphDef *costGraph;
/** Test to see if @c costGraph has been set. */
@property(nonatomic, readwrite) BOOL hasCostGraph;

/** Graphs of the partitions executed by executors. */
@property(nonatomic, readwrite, strong, null_resettable) NSMutableArray<GraphDef*> *partitionGraphsArray;
/** The number of items in @c partitionGraphsArray without causing the array to be created. */
@property(nonatomic, readonly) NSUInteger partitionGraphsArray_Count;

@end

NS_ASSUME_NONNULL_END

CF_EXTERN_C_END

#pragma clang diagnostic pop

// @@protoc_insertion_point(global_scope)
